# üß† Multi-Preference Lambda-weighted DPO (Œª-DPO)

**Œª-DPO** is a novel extension of Direct Preference Optimization (DPO) designed to support **multi-objective alignment** through **listwise human feedback** and **lambda-weighted aggregation**.

## üöÄ Key Features

- **Multi-Preference Optimization**: Incorporates multiple human preference dimensions (e.g., helpfulness, harmlessness, conciseness), each associated with its own listwise preference distribution.
- **Listwise Extension**: Generalizes the DPO loss from pairwise to listwise comparisons, capturing richer ranking information across candidate responses.
- **Lambda-weighted Aggregation**: Introduces a controllable or sampled vector **Œª ‚àà Œî·µê** (the m-dimensional simplex) to combine individual preference losses.
- **Robust Generalization**: By sampling Œª vectors during training, the model learns to generalize across various user preference configurations, enabling dynamic alignment at inference time.

## üßÆ Loss Formulation

Let:
- `x` be an input prompt,
- `{y‚ÇÅ, ..., y_N}` be a set of candidate outputs,
- `œÄ_Œ∏(y|x)` be the current policy,
- `œÄ_ref(y|x)` be a fixed reference policy,
- and `p_i^{*(k)}` be the listwise human preference distribution for the k-th objective.

Each component loss is defined as:

```math
L^{(k)}(Œ∏) = E_{(x, {y_i}) ‚àº D} \left[ 
  - \sum_{i=1}^{N} p_i^{*(k)} \cdot \log \left(
    \frac{\left( \frac{œÄ_Œ∏(y_i|x)}{œÄ_{ref}(y_i|x)} \right)^Œ≤}
         {\sum_{j=1}^{N} \left( \frac{œÄ_Œ∏(y_j|x)}{œÄ_{ref}(y_j|x)} \right)^Œ≤}
  \right) \right]
```

Aggregating all $m$ preferences with a sampled weight vector $\lambda \in \Delta^m$
gives the final training objective:

```math
L(Œ∏) = \mathbb{E}_{\boldsymbol{\lambda} \sim P(\lambda)} \left[\sum_{k=1}^m
\lambda_k \, L^{(k)}(Œ∏)\right]
```

Here $P(\lambda)$ can be any distribution over the simplex (e.g. Dirichlet),
allowing the model to learn from diverse preference trade-offs.

In practice the repository computes $L^{(k)}(Œ∏)$ for each preference dimension
and samples a new $\lambda$ at every training step. The weighted sum directs the
model to outputs that satisfy different user priorities.

## üîß Running Tests

Install the Python dependencies listed in `requirements.txt` and then execute
the following commands inside the `lambda_dpo` directory to verify the
installation:

```bash
cd lambda_dpo
make test
```

For complete usage instructions, please refer to `lambda_dpo/README.md`.
