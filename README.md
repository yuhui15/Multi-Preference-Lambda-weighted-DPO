# ğŸ§  Multi-Preference Lambda-weighted DPO (Î»-DPO)

**Î»-DPO** is a novel extension of Direct Preference Optimization (DPO) designed to support **multi-objective alignment** through **listwise human feedback** and **lambda-weighted aggregation**.

## ğŸš€ Key Features

- **Multi-Preference Optimization**: Incorporates multiple human preference dimensions (e.g., helpfulness, harmlessness, conciseness), each associated with its own listwise preference distribution.
- **Listwise Extension**: Generalizes the DPO loss from pairwise to listwise comparisons, capturing richer ranking information across candidate responses.
- **Lambda-weighted Aggregation**: Introduces a controllable or sampled vector **Î» âˆˆ Î”áµ** (the m-dimensional simplex) to combine individual preference losses.
- **Robust Generalization**: By sampling Î» vectors during training, the model learns to generalize across various user preference configurations, enabling dynamic alignment at inference time.

## ğŸ§® Loss Formulation

Let:
- `x` be an input prompt,
- `{yâ‚, ..., y_N}` be a set of candidate outputs,
- `Ï€_Î¸(y|x)` be the current policy,
- `Ï€_ref(y|x)` be a fixed reference policy,
- and `p_i^{*(k)}` be the listwise human preference distribution for the k-th objective.

Each component loss is defined as:

```math
L^{(k)}(Î¸) = E_{(x, {y_i}) âˆ¼ D} \left[ 
  - \sum_{i=1}^{N} p_i^{*(k)} \cdot \log \left( 
    \frac{\left( \frac{Ï€_Î¸(y_i|x)}{Ï€_{ref}(y_i|x)} \right)^Î²}
         {\sum_{j=1}^{N} \left( \frac{Ï€_Î¸(y_j|x)}{Ï€_{ref}(y_j|x)} \right)^Î²}
  \right) \right]
